{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import httplib2\n",
    "from itertools import zip_longest\n",
    "import bs4\n",
    "import codecs\n",
    "import urllib\n",
    "import re\n",
    "import os\n",
    "from os import walk\n",
    "from time import gmtime, strftime \n",
    "import os.path\n",
    "import ast\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ast import literal_eval\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import socket\n",
    "import socks\n",
    "import pandas.io.sql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Crawl' from 'C:\\\\Users\\\\Wscraper\\\\Documents\\\\WebCrawler\\\\Crawl.py'>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Crawl\n",
    "importlib.reload(Crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo news raw HTML database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.yahoo.com/news\"\n",
    "scrape_date=strftime(\"%Y-%m-%d\", gmtime())\n",
    "        \n",
    "def yahoo_news_meta_data(url, max_count=1000):\n",
    "    df1 = pd.DataFrame()\n",
    "    \n",
    "    response = urllib.request.urlopen(url)\n",
    "    assert response.code == 200\n",
    "    html = response.read()\n",
    "    print(\"reading pages\")\n",
    "    soup = bs4.BeautifulSoup(html,\"html.parser\")\n",
    "    print(url)\n",
    "    results = re.findall(re.compile('href=\"(.+?\\.html)\"'),str(soup))\n",
    "    ad_count=0\n",
    "    for x in results:\n",
    "        if \"news\" in  x and \"https://\" in x:\n",
    "            df1.loc[ad_count,\"Link\"] = x\n",
    "            #print(x)\n",
    "            ad_count+=1\n",
    "        elif \"news\" in x and \"https://\" not in x:\n",
    "            df1.loc[ad_count,\"Link\"] = url+x\n",
    "            #print(x)\n",
    "            ad_count+=1\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pages\n",
      "https://www.yahoo.com/news\n",
      "len(df1):75\n",
      "deleting duplicates\n",
      "len(df2):34\n",
      "len(df2):34\n"
     ]
    }
   ],
   "source": [
    "#default_proxy()    \n",
    "\n",
    "#connect_tor()\n",
    "\n",
    "def scrape_page(url,meta_data):\n",
    "\n",
    "    def get_raw_data(links,max_count=1000):\n",
    "        ad_count= 0\n",
    "        df2 = pd.DataFrame(columns=('Link','Ad_Source'))\n",
    "        for url in links:\n",
    "            try:\n",
    "                response = urllib.request.urlopen(url)\n",
    "                html = response.read()\n",
    "                Ad_Source = bs4.BeautifulSoup(html,\"html.parser\")\n",
    "                Ad_Link= url\n",
    "\n",
    "                df2.loc[ad_count]=[Ad_Link,Ad_Source]\n",
    "                ad_count = ad_count+1\n",
    "            except:\n",
    "                pass\n",
    "                #print(url)\n",
    "\n",
    "        time.sleep(np.random.randint(0,1,1)[0])\n",
    "        return df2\n",
    "\n",
    "    df1 = Crawl.webpage(url,yahoo_news_meta_data)\n",
    "    print(\"len(df1):\"+str(len(df1)))\n",
    "    print(\"deleting duplicates\")\n",
    "    links = df1[['Link']].groupby(['Link']).count().reset_index()['Link']\n",
    "\n",
    "    df2=pd.DataFrame()\n",
    "\n",
    "    #split links into n chunks\n",
    "    chunk_number = 1\n",
    "    if len(links)>chunk_number:\n",
    "        ranges = list(range(0, len(links), len(links) // chunk_number))\n",
    "        split_ = list(zip_longest(ranges, ranges[1:], fillvalue=len(links)))\n",
    "\n",
    "        for i in range(0,len(split_)):\n",
    "            df2= pd.concat([df2,Crawl.webpage(links[split_[i][0]:split_[i][1]],get_raw_data)])\n",
    "            print(\"len(df2):\"+str(len(df2)))\n",
    "    else: \n",
    "        df2= pd.concat([df2,Crawl.webpage(links,get_raw_data)])\n",
    "        print(\"len(df2):\"+str(len(df2)))\n",
    "        \n",
    "    return df2\n",
    "\n",
    "def upload_database(df2,db_name):\n",
    "    #default_proxy() \n",
    "    \n",
    "    scrape_date=strftime(\"%Y-%m-%d\", gmtime())\n",
    "    connection = pymysql.connect(host='127.0.0.1',\n",
    "                             user='rohank',\n",
    "                             password='rohank',\n",
    "                             db='local_database',\n",
    "                             charset='utf8mb4',\n",
    "                             autocommit=True,\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "    with connection.cursor() as cursor:\n",
    "            # Create a new record\n",
    "            sql = \"CREATE TABLE IF NOT EXISTS `\"+db_name+\"` (`Scrape_Date` text,`Website` text,`JSON_Package` json) ;\"\n",
    "            cursor.execute(sql)\n",
    "    connection.commit()\n",
    "\n",
    "    df2['Scrape_Date'] = scrape_date\n",
    "    df2['Website'] = url\n",
    "\n",
    "    print(\"len(df2):\"+str(len(df2)))\n",
    "    for i in range(0,len(df2)):\n",
    "        dict_upload={\n",
    "          \"Link\":str(df2.ix[i].Link),\n",
    "          \"Ad_Source\": str(df2.ix[i].Ad_Source)}\n",
    "        #print(dict_upload)\n",
    "        json_value = json.dumps(dict_upload)\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            # Create a new record\n",
    "            cursor.execute(\"INSERT INTO `\"+db_name+\"` (`Scrape_Date`,`Website`, `JSON_Package`) VALUES (%s,%s,%s)\", (df2.ix[i].Scrape_Date,df2.ix[i].Website,json_value))\n",
    "        connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "    #connect_tor()\n",
    "    \n",
    "url = \"https://www.yahoo.com/news\"\n",
    "pandas_df = scrape_page(url,yahoo_news_meta_data)\n",
    "upload_database(pandas_df,db_name='yahoo_news_database_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo finance top mutual funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=0&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=100&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=200&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=300&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=400&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=500&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=600&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=700&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=800&count=100\n",
      "https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=900&count=100\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pad_Left</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "      <th>Change</th>\n",
       "      <th>Percent_Change</th>\n",
       "      <th>Price</th>\n",
       "      <th>50_Day_Avg</th>\n",
       "      <th>200_Day_Avg</th>\n",
       "      <th>3-Mo Return</th>\n",
       "      <th>YTD Return</th>\n",
       "      <th>Pad_Right</th>\n",
       "      <th>Scrape_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>MIDNX</td>\n",
       "      <td>Matthews India Instl</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.129</td>\n",
       "      <td>31.12</td>\n",
       "      <td>31.22</td>\n",
       "      <td>29.07</td>\n",
       "      <td>0.950</td>\n",
       "      <td>2.033</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>MINDX</td>\n",
       "      <td>Matthews India Investor</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.126</td>\n",
       "      <td>30.95</td>\n",
       "      <td>31.05</td>\n",
       "      <td>28.93</td>\n",
       "      <td>0.944</td>\n",
       "      <td>2.027</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>FSCHX</td>\n",
       "      <td>Fidelity Select Chemicals</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.110</td>\n",
       "      <td>157.50</td>\n",
       "      <td>157.41</td>\n",
       "      <td>157.88</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.951</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>CNPIX</td>\n",
       "      <td>ProFunds Consumer Goods UltraSector Inv</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.110</td>\n",
       "      <td>104.97</td>\n",
       "      <td>106.21</td>\n",
       "      <td>100.72</td>\n",
       "      <td>0.494</td>\n",
       "      <td>1.609</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>CNPSX</td>\n",
       "      <td>ProFunds Consumer Goods UltraSector</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.109</td>\n",
       "      <td>96.31</td>\n",
       "      <td>97.50</td>\n",
       "      <td>92.65</td>\n",
       "      <td>0.469</td>\n",
       "      <td>1.561</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Pad_Left Symbol                                  Company Change  \\\n",
       "0           MIDNX                     Matthews India Instl   0.40   \n",
       "1           MINDX                  Matthews India Investor   0.39   \n",
       "2           FSCHX                Fidelity Select Chemicals   1.73   \n",
       "3           CNPIX  ProFunds Consumer Goods UltraSector Inv   1.15   \n",
       "4           CNPSX      ProFunds Consumer Goods UltraSector   1.05   \n",
       "\n",
       "   Percent_Change   Price 50_Day_Avg 200_Day_Avg  3-Mo Return  YTD Return  \\\n",
       "0           0.129   31.12      31.22       29.07        0.950       2.033   \n",
       "1           0.126   30.95      31.05       28.93        0.944       2.027   \n",
       "2           0.110  157.50     157.41      157.88       -0.061       0.951   \n",
       "3           0.110  104.97     106.21      100.72        0.494       1.609   \n",
       "4           0.109   96.31      97.50       92.65        0.469       1.561   \n",
       "\n",
       "  Pad_Right Scrape_Date  \n",
       "0            2017-07-03  \n",
       "1            2017-07-03  \n",
       "2            2017-07-03  \n",
       "3            2017-07-03  \n",
       "4            2017-07-03  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def crawl_pages(max_pages=1):\n",
    "    scrape_date=strftime(\"%Y-%m-%d\", gmtime())\n",
    "    df = pd.DataFrame(columns=('Pad_Left','Symbol','Company','Change','Percent_Change','Price','50_Day_Avg','200_Day_Avg','3-Mo Return','YTD Return','Pad_Right'))\n",
    "    #def get_stocks():\n",
    "    j=0\n",
    "    i=0\n",
    "    while True:\n",
    "        url = \"https://finance.yahoo.com/screener/predefined/top_mutual_funds?offset=\"+str(i*100)+\"&count=100\"\n",
    "        if i == max_pages:\n",
    "            break\n",
    "        elif i < max_pages:\n",
    "            print(url)\n",
    "            response = urllib.request.urlopen(url)\n",
    "            html = response.read()\n",
    "            soup = bs4.BeautifulSoup(html,\"html.parser\")\n",
    "            table = soup.find(\"table\",{\"class\":re.compile('screener*')})\n",
    "            for row in table.findAll('tr',{\"class\":re.compile('data*')}):\n",
    "                stock_info = [col.getText().strip() for col in row.findAll('td')]\n",
    "                \n",
    "                try:\n",
    "                    assert len(stock_info)==len(df.columns)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                df.loc[j] = [col.getText().strip() for col in row.findAll('td')]\n",
    "\n",
    "                j+=1\n",
    "            i+=1\n",
    "\n",
    "        time.sleep(1)\n",
    "    df['Scrape_Date'] = scrape_date\n",
    "    return df\n",
    "\n",
    "def upload_database(df2,db_name):\n",
    "    #default_proxy() \n",
    "    \n",
    "    scrape_date=strftime(\"%Y-%m-%d\", gmtime())\n",
    "    df2['Scrape_Date'] = scrape_date\n",
    "    \n",
    "    connection_s='mysql+pymysql://rohank:rohank@127.0.0.1:3306/local_database?charset=utf8mb4'\n",
    "    engine = create_engine(connection_s, echo=False)\n",
    "    df2.to_sql(name=db_name, con=engine, if_exists = 'append', chunksize=100, index=False)\n",
    "\n",
    "\n",
    "stock_df = crawl_pages(max_pages=10)\n",
    "\n",
    "def quick_format(x):\n",
    "    try:\n",
    "        return float(str(x).replace('%',''))/10\n",
    "    except:\n",
    "        print(x)\n",
    "        return np.nan\n",
    "\n",
    "stock_df['3-Mo Return'] = stock_df['3-Mo Return'].apply(lambda x: quick_format(x)) \n",
    "stock_df['Percent_Change'] = stock_df['Percent_Change'].apply(lambda x: quick_format(x)) \n",
    "stock_df['YTD Return'] = stock_df['YTD Return'].apply(lambda x: quick_format(x)) \n",
    "\n",
    "upload_database(stock_df,db_name='yahoo_stock_database')\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo news- Extract Transform Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_title(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"h1\",{\"class\":re.compile(\"Lh*\")}).getText().strip()\n",
    "        Field = tmp.replace(r'\\r',' ').replace(r'\\n',' ').replace(r'\\t',' ')\n",
    "    except AttributeError:\n",
    "        Field=\"\"\n",
    "    return Field\n",
    "\n",
    "def get_author(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"a\",{\"class\":re.compile(\"author-link*\")},'itemprop=\"name\"').getText().strip()\n",
    "        Field = tmp.replace(r'\\r',' ').replace(r'\\n',' ').replace(r'\\t',' ')\n",
    "    except AttributeError:\n",
    "        Field=\"\"\n",
    "    return Field\n",
    "\n",
    "def get_post_date(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"time\",{\"class\":re.compile(\"Fz*\")},'itemprop=\"datePublished\"')[\"datetime\"]\n",
    "        Field = tmp.replace(r'\\r','').replace(r'\\n','').replace(r'\\t','')\n",
    "        Field= datetime.datetime.strptime(Field,'%Y-%m-%dT%H:%M:%S.%fZ').date()\n",
    "    except AttributeError:\n",
    "        Field=\"NULL\"\n",
    "    return Field\n",
    "\n",
    "\n",
    "def get_post_time(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"time\",{\"class\":re.compile(\"Fz*\")},'itemprop=\"datePublished\"')[\"datetime\"]\n",
    "        Field = tmp.replace(r'\\r','').replace(r'\\n','').replace(r'\\t','')\n",
    "        Field= datetime.datetime.strptime(Field,'%Y-%m-%dT%H:%M:%S.%fZ').time()\n",
    "    except AttributeError:\n",
    "        Field=\"NULL\"\n",
    "    return Field\n",
    "\n",
    "def get_post(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"div\",{\"class\":re.compile(\"canvas-body*\")}).getText().strip()\n",
    "        Field = tmp.replace(r'\\r',' ').replace(r'\\n',' ').replace(r'\\t',' ')\n",
    "    except AttributeError:\n",
    "        Field=\"NULL\"\n",
    "    return Field\n",
    "\n",
    "def get_provider(s):\n",
    "    s = s.decode('utf-8')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        tmp=soup1.find(\"span\",{\"class\":re.compile(\"provider*\")}).getText().strip()\n",
    "        Field = tmp.replace(r'\\r',' ').replace(r'\\n',' ').replace(r'\\t',' ')\n",
    "    except AttributeError:\n",
    "        Field=\"NULL\"\n",
    "    return Field\n",
    "\n",
    "def get_images(s):\n",
    "    s = s.decode('ascii','ignore')\n",
    "    soup1=bs4.BeautifulSoup(s,\"html.parser\")\n",
    "    try:\n",
    "        Poster_images=[]\n",
    "        tmp=soup1.findAll(\"img\")\n",
    "        for each in tmp:\n",
    "            Poster_images.append(each[\"src\"])\n",
    "    except AttributeError:\n",
    "        Poster_images=\"NULL\"\n",
    "    return str(Poster_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset:  0\n",
      "chunk size: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extract_pipeline = [(\"Post\",get_post),\n",
    "                    (\"Images\",get_images),\n",
    "                    (\"Author\",get_author),\n",
    "                    (\"Provider\",get_provider),\n",
    "                    (\"Title\",get_title),\n",
    "                    (\"Post_Date\",get_post_date),\n",
    "                    (\"Post_Time\",get_post_time)]\n",
    "\n",
    "connection_s='mysql+pymysql://rohank:rohank@127.0.0.1:3306/local_database?charset=utf8mb4'\n",
    "engine = create_engine(connection_s, echo=False)\n",
    "\n",
    "\n",
    "to_db_name = \"yahoo_news_database_ads\"\n",
    "from_db_name = \"yahoo_news_database_raw\"\n",
    "chunk_size = 10\n",
    "try:\n",
    "    sql = \"SELECT count(*) as set_point FROM \"+to_db_name;\n",
    "    offset = psql.read_sql(sql, engine)['set_point'].ix[0]\n",
    "except:\n",
    "    offset=0\n",
    "print(\"offset: \",offset)\n",
    "print(\"chunk size:\",chunk_size)\n",
    "\n",
    "def encode_utf8(s):\n",
    "    return codecs.encode(s, 'utf-8')\n",
    "\n",
    "while True:\n",
    "    sql = \"SELECT * FROM \"+from_db_name+\" limit %d offset %d\" % (chunk_size,offset) \n",
    "    chunk=psql.read_sql(sql, engine)\n",
    "\n",
    "\n",
    "    d = ast.literal_eval(chunk[\"JSON_Package\"][0])\n",
    "    #print(codecs.encode(d['Ad_Source'],'utf-8'))\n",
    "    chunk['JSON_Package']=chunk['JSON_Package'].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "\n",
    "    chunk['Link'] = chunk['JSON_Package'].apply(lambda x: x['Link'])\n",
    "    chunk['Ad_Source'] = chunk['JSON_Package'].apply(lambda x: x['Ad_Source'])\n",
    "    chunk = chunk.drop([\"JSON_Package\"],axis=1)    \n",
    "    chunk['Ad_Source']=chunk['Ad_Source'].apply(lambda x: encode_utf8(x))\n",
    "    \n",
    "    for field_name,function in extract_pipeline:\n",
    "        chunk[field_name]=chunk['Ad_Source'].apply(lambda x: function(x))\n",
    "    #chunk['Author']=chunk['Ad_Source'].apply(lambda x: get_author(x))\n",
    "    chunk = chunk.drop([\"Ad_Source\"],axis=1)   \n",
    "    #print(chunk['Ad_Source'][0])\n",
    "\n",
    "    #print(chunk)\n",
    "    \n",
    "    chunk.to_sql(name=to_db_name, con=engine, if_exists = 'append', chunksize=10, index=False)\n",
    "    offset += chunk_size\n",
    "    if len(chunk) < chunk_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scrape_Date</th>\n",
       "      <th>Website</th>\n",
       "      <th>Link</th>\n",
       "      <th>Post</th>\n",
       "      <th>Images</th>\n",
       "      <th>Author</th>\n",
       "      <th>Provider</th>\n",
       "      <th>Title</th>\n",
       "      <th>Post_Date</th>\n",
       "      <th>Post_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>https://www.yahoo.com/news</td>\n",
       "      <td>https://www.yahoo.com/news/news/bill-create-pa...</td>\n",
       "      <td>For months, House Minority Leader Nancy Pelosi...</td>\n",
       "      <td>['https://s.yimg.com/uu/api/res/1.2/EgCSY2qYak...</td>\n",
       "      <td>Michael Isikoff</td>\n",
       "      <td>Yahoo News</td>\n",
       "      <td>Bill to create panel that could remove Trump f...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>12:45:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>https://www.yahoo.com/news</td>\n",
       "      <td>https://www.yahoo.com/news/news/body-missing-5...</td>\n",
       "      <td>The Los Angeles County Sheriff’s Department co...</td>\n",
       "      <td>['https://s.yimg.com/ny/api/res/1.2/NwJPdDKIEs...</td>\n",
       "      <td></td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>Body Of Missing 5-Year-Old Boy Found Near Lake...</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>07:33:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>https://www.yahoo.com/news</td>\n",
       "      <td>https://www.yahoo.com/news/news/chicago-school...</td>\n",
       "      <td>CHICAGO (Reuters) - The cash-strapped Chicago ...</td>\n",
       "      <td>['https://s.yimg.com/ny/api/res/1.2/1v57GsbtrM...</td>\n",
       "      <td></td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Chicago schools make partial payment to teache...</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>00:41:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>https://www.yahoo.com/news</td>\n",
       "      <td>https://www.yahoo.com/news/news/chinas-heavy-l...</td>\n",
       "      <td>China's new heavy-lift rocket launch fails in ...</td>\n",
       "      <td>['https://s.yimg.com/g/images/spaceball.gif', ...</td>\n",
       "      <td></td>\n",
       "      <td>Yahoo News Video</td>\n",
       "      <td>China's new heavy-lift rocket launch fails in ...</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>18:24:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>https://www.yahoo.com/news</td>\n",
       "      <td>https://www.yahoo.com/news/news/congress-cool-...</td>\n",
       "      <td>PORTLAND, Maine (AP) -- The summer air is sizz...</td>\n",
       "      <td>['https://s.yimg.com/ny/api/res/1.2/jFJ4GVeXE6...</td>\n",
       "      <td></td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>Congress is cool to Trump's proposal to end he...</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>23:59:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scrape_Date                     Website  \\\n",
       "0  2017-07-03  https://www.yahoo.com/news   \n",
       "1  2017-07-03  https://www.yahoo.com/news   \n",
       "2  2017-07-03  https://www.yahoo.com/news   \n",
       "3  2017-07-03  https://www.yahoo.com/news   \n",
       "4  2017-07-03  https://www.yahoo.com/news   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.yahoo.com/news/news/bill-create-pa...   \n",
       "1  https://www.yahoo.com/news/news/body-missing-5...   \n",
       "2  https://www.yahoo.com/news/news/chicago-school...   \n",
       "3  https://www.yahoo.com/news/news/chinas-heavy-l...   \n",
       "4  https://www.yahoo.com/news/news/congress-cool-...   \n",
       "\n",
       "                                                Post  \\\n",
       "0  For months, House Minority Leader Nancy Pelosi...   \n",
       "1  The Los Angeles County Sheriff’s Department co...   \n",
       "2  CHICAGO (Reuters) - The cash-strapped Chicago ...   \n",
       "3  China's new heavy-lift rocket launch fails in ...   \n",
       "4  PORTLAND, Maine (AP) -- The summer air is sizz...   \n",
       "\n",
       "                                              Images           Author  \\\n",
       "0  ['https://s.yimg.com/uu/api/res/1.2/EgCSY2qYak...  Michael Isikoff   \n",
       "1  ['https://s.yimg.com/ny/api/res/1.2/NwJPdDKIEs...                    \n",
       "2  ['https://s.yimg.com/ny/api/res/1.2/1v57GsbtrM...                    \n",
       "3  ['https://s.yimg.com/g/images/spaceball.gif', ...                    \n",
       "4  ['https://s.yimg.com/ny/api/res/1.2/jFJ4GVeXE6...                    \n",
       "\n",
       "                       Provider  \\\n",
       "0                    Yahoo News   \n",
       "1  International Business Times   \n",
       "2                       Reuters   \n",
       "3              Yahoo News Video   \n",
       "4              Associated Press   \n",
       "\n",
       "                                               Title  Post_Date Post_Time  \n",
       "0  Bill to create panel that could remove Trump f... 2017-06-30  12:45:21  \n",
       "1  Body Of Missing 5-Year-Old Boy Found Near Lake... 2017-07-02  07:33:22  \n",
       "2  Chicago schools make partial payment to teache... 2017-07-01  00:41:36  \n",
       "3  China's new heavy-lift rocket launch fails in ... 2017-07-02  18:24:27  \n",
       "4  Congress is cool to Trump's proposal to end he... 2017-07-02  23:59:42  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(to_db_name, con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
